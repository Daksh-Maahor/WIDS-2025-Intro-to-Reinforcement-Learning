{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bandit Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524ad9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Bandit:\n",
    "    def __init__(self, mean):\n",
    "        self.mean = mean\n",
    "    def pullLever(self):\n",
    "        return np.random.normal(self.mean, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bandit is one option (or “arm”) you can choose, where the reward you get is uncertain and must be learned by trying it out.\n",
    "In multi-armed bandits, you repeatedly pick among several such uncertain options to find which one pays best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A list of ten bandit objects initialized in the list..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4669201d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bandits = [Bandit(m) for m in np.random.normal(0, 1, 10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate reward from that bandit, use the pullLever() command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a5f8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example pull\n",
    "bandits[0].pullLever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greedy algorithm Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac506387",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_greedy():\n",
    "    N = np.zeros(10)\n",
    "    Q = np.zeros(10)\n",
    "    rewards = []\n",
    "    for _ in range()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the cumulative average of rewards as the number of iterations increases. and display that image below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6435472537755328,\n",
       " 0.31353277992774264,\n",
       " -0.6773464337428614,\n",
       " 0.3091041958052412,\n",
       " -1.6368502727161278,\n",
       " -0.25570078069403934,\n",
       " 0.09220863603566876,\n",
       " 1.967504639746934,\n",
       " 2.211827696069065,\n",
       " -0.28057660543779944]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for _ in range(1000):\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\epsilon$-greedy Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epsilon_greedy(epsilon):\n",
    "    # TODO: Implement the epsilon greedy algorithm here\n",
    "    # Return the reward from the bandits in a list\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the cumulative average of rewards as the number of iterations increases but for various values of $\\epsilon$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the optimal $\\epsilon$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the $\\epsilon$-greedy algorithm for 1000 iterations and find the optimal $\\epsilon$ value by plotting the cumulative average of rewards for various values of $\\epsilon$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c17429",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy(bandits, eps, iters=1000):\n",
    "    n = len(bandits)\n",
    "    counts = np.zeros(n)\n",
    "    values = np.zeros(n)\n",
    "    rewards = []\n",
    "    for t in range(iters):\n",
    "        if np.random.rand() < eps:\n",
    "            a = np.random.randint(n)\n",
    "        else:\n",
    "            a = np.argmax(values)\n",
    "        r = bandits[a].pullLever()\n",
    "        counts[a] += 1\n",
    "        values[a] += (r - values[a]) / counts[a]\n",
    "        rewards.append(r)\n",
    "    return np.cumsum(rewards) / (np.arange(iters) + 1)\n",
    "\n",
    "epsilons = [0, 0.01, 0.1, 0.2]\n",
    "for e in epsilons:\n",
    "    plt.plot(epsilon_greedy(bandits, e), label=f\"eps={e}\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimistic Initial Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f620e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimistic(bandits, init=5, eps=0.1, iters=1000):\n",
    "    n = len(bandits)\n",
    "    counts = np.zeros(n)\n",
    "    values = np.ones(n) * init\n",
    "    rewards = []\n",
    "    for t in range(iters):\n",
    "        if np.random.rand() < eps:\n",
    "            a = np.random.randint(n)\n",
    "        else:\n",
    "            a = np.argmax(values)\n",
    "        r = bandits[a].pullLever()\n",
    "        counts[a] += 1\n",
    "        values[a] += (r - values[a]) / counts[a]\n",
    "        rewards.append(r)\n",
    "    return np.cumsum(rewards) / (np.arange(iters) + 1)\n",
    "\n",
    "plt.plot(optimistic(bandits), label=\"Optimistic\")\n",
    "plt.plot(epsilon_greedy(bandits, 0.1), label=\"Epsilon=0.1\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the cumulative average of rewards as the number of iterations increases for an optimistic greedy of $Q_1 = 10$ and a non-optimistic $\\epsilon = 0.1$ and try to compare which is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upper Confidence Bound (UCB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ca437e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ucb(bandits, c=2, iters=1000):\n",
    "    n = len(bandits)\n",
    "    counts = np.zeros(n)\n",
    "    values = np.zeros(n)\n",
    "    rewards = []\n",
    "    for t in range(iters):\n",
    "        if 0 in counts:\n",
    "            a = np.argmin(counts)\n",
    "        else:\n",
    "            ucb_vals = values + c * np.sqrt(np.log(t+1) / counts)\n",
    "            a = np.argmax(ucb_vals)\n",
    "        r = bandits[a].pullLever()\n",
    "        counts[a] += 1\n",
    "        values[a] += (r - values[a]) / counts[a]\n",
    "        rewards.append(r)\n",
    "    return np.cumsum(rewards) / (np.arange(iters) + 1)\n",
    "\n",
    "plt.plot(ucb(bandits), label=\"UCB\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
